{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n",
      "n_samples: 3387, n_features: 24545\n",
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, init_size=1000, n_clusters=4, n_init=1)\n",
      "Cluster 0:\n",
      " space\n",
      " henry\n",
      " toronto\n",
      " access\n",
      " nasa\n",
      " com\n",
      " digex\n",
      " pat\n",
      " gov\n",
      " alaska\n",
      "Cluster 1:\n",
      " graphics\n",
      " space\n",
      " image\n",
      " com\n",
      " nasa\n",
      " university\n",
      " posting\n",
      " images\n",
      " program\n",
      " file\n",
      "Cluster 2:\n",
      " god\n",
      " people\n",
      " com\n",
      " jesus\n",
      " don\n",
      " say\n",
      " believe\n",
      " think\n",
      " bible\n",
      " just\n",
      "Cluster 3:\n",
      " sgi\n",
      " livesey\n",
      " keith\n",
      " wpd\n",
      " solntze\n",
      " jon\n",
      " com\n",
      " caltech\n",
      " morality\n",
      " moral\n",
      "First method:\n",
      "Homogeneity: 0.584\n",
      "Completeness: 0.632\n",
      "V-measure: 0.607\n",
      "Adjusted Rand-Index: 0.602\n",
      "Silhouette Coefficient: 0.416 \n",
      "Second method:\n",
      "Cluster 0:\n",
      " space\n",
      " henry\n",
      " toronto\n",
      " access\n",
      " nasa\n",
      " com\n",
      " digex\n",
      " pat\n",
      " gov\n",
      " alaska\n",
      "Cluster 1:\n",
      " graphics\n",
      " space\n",
      " image\n",
      " com\n",
      " nasa\n",
      " university\n",
      " posting\n",
      " images\n",
      " program\n",
      " file\n",
      "Cluster 2:\n",
      " god\n",
      " people\n",
      " com\n",
      " jesus\n",
      " don\n",
      " say\n",
      " believe\n",
      " think\n",
      " bible\n",
      " just\n",
      "Cluster 3:\n",
      " sgi\n",
      " livesey\n",
      " keith\n",
      " wpd\n",
      " solntze\n",
      " jon\n",
      " com\n",
      " caltech\n",
      " morality\n",
      " moral\n",
      "Homogeneity: 0.584\n",
      "Completeness: 0.632\n",
      "V-measure: 0.607\n",
      "Adjusted Rand-Index: 0.602\n",
      "Silhouette Coefficient: 0.427 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [\n",
    "'alt.atheism',\n",
    "'talk.religion.misc',\n",
    "'comp.graphics',\n",
    "'sci.space',\n",
    "]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# warnings imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "#import dataset\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "shuffle=True, random_state=42)\n",
    "#save labels\n",
    "labels = dataset.target\n",
    "#get the unique labels\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "data = dataset.data\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english',\n",
    "use_idf=True)\n",
    "X = vectorizer.fit_transform(data)\n",
    "#The X object is now our input vector which contains the TF-IDF representation of our\n",
    "#dataset. \n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "\n",
    "#Dimensionality Reduction\n",
    "# Vectorizer results are normalized, which makes KMeans behave better\n",
    "    # Since LSA/SVD results are not normalized, we have to redo the normalization.\n",
    "\n",
    "    #If we do not normalize the data, variables with different scaling \n",
    "    # will be weighted differently in the distance formula \n",
    "    # that is being optimized during training.\n",
    "\t\n",
    "\n",
    "n_components = 5 #Sets the number of latent dimensions (topics) to which the data is reduced. \n",
    "                  #This controls how much the dimensionality of the dataset is reduced.\n",
    "#Performs truncated singular value decomposition (SVD) on the input matrix \n",
    "#X to reduce its dimensionality.\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "#Combines the SVD and normalization steps into a single pipeline for streamlined processing. \n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "#The final X is the input which we will be using. \n",
    "# It has been cleaned, TF-IDF transformed, and its dimensions reduced.\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "#scikit-learn offers two implementations of kmeans:\n",
    "# either in mini-batches or without\n",
    "minibatch = True\n",
    "if minibatch:\n",
    "   km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "   init_size=1000, batch_size=1000)\n",
    "else:\n",
    "   km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X)\n",
    "# top words per cluster\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "   print(\"Cluster %d:\" % i)\n",
    "   for ind in order_centroids[i, :10]:\n",
    "      print(' %s' % terms[ind])\n",
    "print(\"First method:\")\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "#Note: You might see different results, as machine learning \n",
    "# algorithms do not produce the exact same results each time.\n",
    "#km.predict(X_test) to test our model\n",
    "\n",
    "#imports the KMeans algorithm from the scikit-learn library and \n",
    "# creates an instance of it with three clusters, a random state of 0, \n",
    "# and automatic initialization\n",
    "#KMeans algorithm is a clustering algorithm that groups \n",
    "# similar data points together based on their distance from each other\n",
    "\n",
    "#random runs: This affects how the initial cluster centroids are chosen \n",
    "#and ensures consistent results across multiple runs.\n",
    "#n_init=auto: Automatically runs 10 initializations and picks the best one based on inertia (objective function).\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 0, n_init='auto')\n",
    "#The fit method is then called on the normalized training data \n",
    "# to train the KMeans model on the data.\n",
    "kmeans.fit(X)\n",
    "print(\"Second method:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "   print(\"Cluster %d:\" % i)\n",
    "   for ind in order_centroids[i, :10]:\n",
    "      print(' %s' % terms[ind])\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n",
      "n_samples: 3387, n_features: 24545\n",
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, init_size=1000, n_clusters=4, n_init=1)\n",
      "Cluster 0:\n",
      " space\n",
      " earth\n",
      " planet\n",
      " nasa\n",
      " venus\n",
      " spacecraft\n",
      " solar\n",
      " god\n",
      " like\n",
      " surface\n",
      "Cluster 1:\n",
      " graphics\n",
      " image\n",
      " pub\n",
      " jpeg\n",
      " ftp\n",
      " data\n",
      " available\n",
      " mail\n",
      " space\n",
      " file\n",
      "Cluster 2:\n",
      " space\n",
      " graphics\n",
      " god\n",
      " pub\n",
      " com\n",
      " earth\n",
      " people\n",
      " mail\n",
      " jesus\n",
      " like\n",
      "Cluster 3:\n",
      " god\n",
      " space\n",
      " jesus\n",
      " jehovah\n",
      " people\n",
      " earth\n",
      " lord\n",
      " elohim\n",
      " like\n",
      " com\n",
      "First method:\n",
      "Homogeneity: 0.444\n",
      "Completeness: 0.440\n",
      "V-measure: 0.442\n",
      "Adjusted Rand-Index: 0.433\n",
      "Silhouette Coefficient: 0.307 \n",
      "Second method:\n",
      "Cluster 0:\n",
      " space\n",
      " earth\n",
      " planet\n",
      " nasa\n",
      " venus\n",
      " spacecraft\n",
      " solar\n",
      " god\n",
      " surface\n",
      " like\n",
      "Cluster 1:\n",
      " graphics\n",
      " pub\n",
      " mail\n",
      " space\n",
      " com\n",
      " ftp\n",
      " ray\n",
      " send\n",
      " 128\n",
      " 3d\n",
      "Cluster 2:\n",
      " image\n",
      " jpeg\n",
      " graphics\n",
      " data\n",
      " file\n",
      " images\n",
      " available\n",
      " space\n",
      " ftp\n",
      " pub\n",
      "Cluster 3:\n",
      " god\n",
      " space\n",
      " jesus\n",
      " earth\n",
      " people\n",
      " jehovah\n",
      " graphics\n",
      " com\n",
      " lord\n",
      " like\n",
      "Homogeneity: 0.446\n",
      "Completeness: 0.471\n",
      "V-measure: 0.458\n",
      "Adjusted Rand-Index: 0.434\n",
      "Silhouette Coefficient: 0.367 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Load SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import dataset\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Save labels\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "data = dataset.data\n",
    "\n",
    "# Vectorize data using Bag-of-Words (BoW)\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "n_components = 5\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "# KMeans Clustering\n",
    "minibatch = True\n",
    "if minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "km.fit(X)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "\n",
    "# Top words per cluster\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "\n",
    "# Metrics for the first method\n",
    "print(\"First method:\")\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \" % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "# KMeans with different initialization\n",
    "kmeans = KMeans(n_clusters=true_k, random_state=0, n_init='auto')\n",
    "kmeans.fit(X)\n",
    "\n",
    "print(\"Second method:\")\n",
    "original_space_centroids = svd.inverse_transform(kmeans.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, kmeans.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, kmeans.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, kmeans.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, kmeans.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, kmeans.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n",
      "n_samples: 3387, n_features: 24545\n",
      "Clustering sparse data with Naive Bayes\n",
      "Cluster 0:\n",
      " com\n",
      " god\n",
      " people\n",
      " don\n",
      " space\n",
      " article\n",
      " just\n",
      " think\n",
      " sandvik\n",
      " like\n",
      "Cluster 1:\n",
      " sandvik\n",
      " god\n",
      " kent\n",
      " apple\n",
      " sgi\n",
      " livesey\n",
      " newton\n",
      " jesus\n",
      " keith\n",
      " morality\n",
      "Cluster 2:\n",
      " sgi\n",
      " livesey\n",
      " keith\n",
      " solntze\n",
      " wpd\n",
      " jon\n",
      " caltech\n",
      " morality\n",
      " moral\n",
      " objective\n",
      "Cluster 3:\n",
      " henry\n",
      " toronto\n",
      " sandvik\n",
      " zoo\n",
      " spencer\n",
      " zoology\n",
      " kent\n",
      " apple\n",
      " livesey\n",
      " sgi\n",
      "First method:\n",
      "Homogeneity: 0.113\n",
      "Completeness: 0.199\n",
      "V-measure: 0.144\n",
      "Adjusted Rand-Index: 0.097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Fetch dataset\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "data = dataset.data\n",
    "labels = dataset.target\n",
    "\n",
    "print(f\"{len(data)} documents\")\n",
    "print(f\"{len(dataset.target_names)} categories\")\n",
    "\n",
    "# Vectorize data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(f\"n_samples: {X.shape[0]}, n_features: {X.shape[1]}\")\n",
    "\n",
    "# Dimensionality reduction with SVD (without normalization)\n",
    "n_components = 5\n",
    "svd = TruncatedSVD(n_components)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "# Ensure non-negative input for MultinomialNB\n",
    "X_reduced = np.abs(X_reduced)  # Take absolute values\n",
    "\n",
    "# Train Naive Bayes on the entire dataset\n",
    "model = MultinomialNB()\n",
    "model.fit(X_reduced, labels)\n",
    "\n",
    "# Predict the same dataset to simulate clustering\n",
    "predicted_labels = model.predict(X_reduced)\n",
    "\n",
    "# Top words for each cluster\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "original_space_centroids = svd.inverse_transform(np.eye(n_components))\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "print(\"Clustering sparse data with Naive Bayes\")\n",
    "for i in range(len(dataset.target_names)):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f' {terms[ind]}')\n",
    "\n",
    "# Metrics\n",
    "print(\"First method:\")\n",
    "print(f\"Homogeneity: {metrics.homogeneity_score(labels, predicted_labels):0.3f}\")\n",
    "print(f\"Completeness: {metrics.completeness_score(labels, predicted_labels):0.3f}\")\n",
    "print(f\"V-measure: {metrics.v_measure_score(labels, predicted_labels):0.3f}\")\n",
    "print(f\"Adjusted Rand-Index: {metrics.adjusted_rand_score(labels, predicted_labels):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n",
      "n_samples: 3387, n_features: 24545\n",
      "Clustering sparse data with Naive Bayes\n",
      "Cluster 0:\n",
      " jpeg\n",
      " image\n",
      " file\n",
      " gif\n",
      " images\n",
      " color\n",
      " format\n",
      " graphics\n",
      " available\n",
      " version\n",
      "Cluster 1:\n",
      " graphics\n",
      " data\n",
      " pub\n",
      " ftp\n",
      " mail\n",
      " space\n",
      " 128\n",
      " god\n",
      " com\n",
      " jehovah\n",
      "Cluster 2:\n",
      " jehovah\n",
      " god\n",
      " elohim\n",
      " lord\n",
      " jesus\n",
      " christ\n",
      " father\n",
      " mcconkie\n",
      " said\n",
      " unto\n",
      "Cluster 3:\n",
      " space\n",
      " earth\n",
      " planet\n",
      " spacecraft\n",
      " venus\n",
      " solar\n",
      " launch\n",
      " surface\n",
      " moon\n",
      " nasa\n",
      "Homogeneity: 0.425\n",
      "Completeness: 0.478\n",
      "V-measure: 0.450\n",
      "Adjusted Rand-Index: 0.484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Fetch dataset\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "data = dataset.data\n",
    "labels = dataset.target\n",
    "\n",
    "print(f\"{len(data)} documents\")\n",
    "print(f\"{len(dataset.target_names)} categories\")\n",
    "\n",
    "# Vectorize data using Bag-of-Words (BoW)\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(f\"n_samples: {X.shape[0]}, n_features: {X.shape[1]}\")\n",
    "\n",
    "# Dimensionality reduction with SVD (without normalization)\n",
    "n_components = 5\n",
    "svd = TruncatedSVD(n_components)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "# Ensure non-negative input for MultinomialNB\n",
    "X_reduced = np.abs(X_reduced)  # Take absolute values\n",
    "\n",
    "# Train Naive Bayes on the entire dataset\n",
    "model = MultinomialNB()\n",
    "model.fit(X_reduced, labels)\n",
    "\n",
    "# Predict the same dataset to simulate clustering\n",
    "predicted_labels = model.predict(X_reduced)\n",
    "\n",
    "# Top words for each cluster\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "original_space_centroids = svd.inverse_transform(np.eye(n_components))\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "print(\"Clustering sparse data with Naive Bayes\")\n",
    "for i in range(len(dataset.target_names)):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f' {terms[ind]}')\n",
    "\n",
    "# Metrics\n",
    "print(f\"Homogeneity: {metrics.homogeneity_score(labels, predicted_labels):0.3f}\")\n",
    "print(f\"Completeness: {metrics.completeness_score(labels, predicted_labels):0.3f}\")\n",
    "print(f\"V-measure: {metrics.v_measure_score(labels, predicted_labels):0.3f}\")\n",
    "print(f\"Adjusted Rand-Index: {metrics.adjusted_rand_score(labels, predicted_labels):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Space exploration is the future of humanity.\n",
      "Predicted Cluster: 0\n",
      "\n",
      "Text: Graphics design is evolving with AI.\n",
      "Predicted Cluster: 1\n",
      "\n",
      "Text: The religious history has many perspectives.\n",
      "Predicted Cluster: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_data = [\n",
    "    \"Space exploration is the future of humanity.\",\n",
    "    \"Graphics design is evolving with AI.\",\n",
    "    \"The religious history has many perspectives.\",\n",
    "]\n",
    "\n",
    "new_data_tfidf = vectorizer.transform(new_data)\n",
    "\n",
    "\n",
    "new_data_lsa = lsa.transform(new_data_tfidf)\n",
    "\n",
    "predicted_clusters = km.predict(new_data_lsa)\n",
    "\n",
    "\n",
    "for text, cluster in zip(new_data, predicted_clusters):\n",
    "    print(f\"Text: {text}\\nPredicted Cluster: {cluster}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
